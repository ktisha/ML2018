%% -*- TeX-engine: luatex; ispell-language: "russian" -*-

\documentclass[a4paper,12pt]{article}
\usepackage{subcaption}
\input{../handout-base}

\begin{document}
\subsection*{Домашнее задание №4: <<Байес на страже SMS>>}

\begin{tabular}{@{}lr}
  \textbf{Дедлайн 1} (20 баллов): & 19 марта, 23:59 \\
  \textbf{Дедлайн 2} (10 баллов): & 26 марта, 23:59
\end{tabular}

Домашнее задание нужно написать на Python и сдать в виде одного файла.
Правило именования файла: \texttt{name\_surname\_4.[py | ipnb]}. Например, если
вас зовут Иван Петров, то имя файла должно быть: \texttt{ivan\_petrov\_4.py} или \texttt{ivan\_petrov\_4.ipnb}.

\makebox[\linewidth]{\hrulefill}

\begin{figure}[h!]
  \centering
  \includegraphics[width=.8\linewidth]{images/spam}
\end{figure}

В данном домашнем задании предлагается реализовать наивный Байесов классификатор для определения спама в SMS-сообщении. По ссылке \footnote{\url{https://gist.github.com/ktisha/6951a0b85cf040cdeb46819e51fb62dd}} находится датасет, размеченных сообщений. Первое слово строки это идентификатор класса spam или ham, далее через табуляцию следует сообщение.

\paragraph{1} Прежде чем строить классификатор нужно привести данные в удобный для классификации формат. Для этого воспользуемся стандартной моделью для текстов под названием Bag of words \footnote{\url{https://en.wikipedia.org/wiki/Bag-of-words_model}}.\\ Реализуйте функцию \pythoninline{vectorize}, принимающую на вход список строк длины N и возвращающую матрицу размера (N, M), где M размер словаря для входных данных. В качестве словаря будем использовать все слова, которые встречаются в переданном массиве. 
В каждой строке матрицы на $j$-м позиции находится число $x$, которое означает, что $j$-е слово встретилось в сообщении $x$ раз.

\paragraph{2} Наивный Байесов классификатор работает в предположении о независимости признаков объекта. В нашем случае это означает, что вероятность встретить некоторое слово в сообщении не зависит от наличия других слов в этом сообщении.  \\
Т.к. мы векторизовали сообщения с учётом частот встречаемости слов, мы будем использовать мультиномиальную модель классификатора и  оценки будут несколько отличаться от тех, которые использовались на лекции. \\
\clearpage
Оценку для параметров для каждого из признаков можно записать следующим образом:\\
$$\theta_{yj} = \frac{\sum\limits_{i=1}^l [y_i = y] x_{ij}}{\sum\limits_{j \in V}\sum\limits_{i=1}^l [y_i = y] x_{ij}}$$\\
$x_{ij}$ -- значение $j$-го признака объекта $i$\\
$V$ -- словарь входных данных.\\

Другими словами, числитель описывает сколько раз слово встречается в сообщениях класса $y$ (включая повторы), а знаменатель – это суммарное количество слов во всех документах этого класса.

Реализуйте метод \pythoninline{fit}, который по переданной выборке вычисляет следующие параметры, которые понадобятся на этапе классификации:
\begin{enumerate}
  \item оценка априорной вероятности классов $\hat{P_y}$
  \item относительные частоты слов для каждого класса
  \item суммарное количество слов для сообщений каждого класса
  \item размер словаря выборки
\end{enumerate}
\bigbreak
\begin{python3}
class NaiveBayes:
    def __init(self, alpha):
        self.alpha = alpha
        ... 
        
    def fit(self, X, y):        
        ...
      
    def predict(self, X):
        ...    
        
    def score(self, X, y):
        ... 
        
\end{python3}

\paragraph{4} Реализуйте метод \pythoninline{predict}, принимающий массив объектов \pythoninline{X} и возвращающий список соответствующих меток классов. 
Алгоритм классификации выглядит следующим образом:
$$a(x) = \arg\max\limits_{y \in Y}  P_y \prod\limits_{j=1}^{|V|} p(x^j |y)$$
Однако при достаточно большой длине сообщения придётся перемножать большое количество очень маленьких чисел. Стандартный способ избежать арифметического переполнения снизу\footnote{\url{https://en.wikipedia.org/wiki/Arithmetic_underflow}} -- применение логарифма к выражению, стоящему под $\arg\max$. Таким образом формула для нашего алгоритма переписывается следующим образом:\\
$$a(x) = \arg\max\limits_{y \in Y} [ \log(P_y) + \sum\limits_{j=1}^{|V|} log(p(x^j |y))]$$

Для решения проблемы неизвестных слов при классификации воспользуйтесь сглаживанием Лапласа для $p(x^j | y)$.
В нашей модели данных аддитивное сглаживание\footnote{\url{https://en.wikipedia.org/wiki/Additive_smoothing}} выражается в добавлении $\alpha$ в числитель и $\alpha * |V|$ в знаменатель выражения из пункта 2.

\paragraph{6} Реализуйте метод \pythoninline{score} для оценки работы классификатора, вычисляющий процент правильно классифицированных объектов.

\end{document}
